{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b168d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for 600025.SH from 20200101 to 20241231...\n",
      "Successfully fetched 1212 rows of raw data.\n",
      "Original shape: (1212, 8)\n",
      "Shape after dropping NaNs: (1190, 8)\n",
      "Prepared X_raw shape: (1190, 5) (cleaned raw data)\n",
      "Prepared y shape: (1190,)\n",
      "\n",
      "--- Starting Manual Grid Search across Feature Timeperiods ---\n",
      "\n",
      "Processing timeperiod: 10\n",
      "Shape before dropping NaNs (TP=10): (1190, 13)\n",
      "Shape after dropping NaNs (TP=10): (1171, 13)\n",
      "Cleaned X shape (TP=10): (1171, 12), y shape: (1171,)\n",
      "Running GridSearchCV for C and kernel (TP=10)...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best params for TP=10: {'svm__C': 1, 'svm__kernel': 'linear'}\n",
      "Best F1 score for TP=10: 0.5226\n",
      "\n",
      "Processing timeperiod: 14\n",
      "Shape before dropping NaNs (TP=14): (1190, 13)\n",
      "Shape after dropping NaNs (TP=14): (1171, 13)\n",
      "Cleaned X shape (TP=14): (1171, 12), y shape: (1171,)\n",
      "Running GridSearchCV for C and kernel (TP=14)...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best params for TP=14: {'svm__C': 0.1, 'svm__kernel': 'linear'}\n",
      "Best F1 score for TP=14: 0.4984\n",
      "\n",
      "Processing timeperiod: 20\n",
      "Shape before dropping NaNs (TP=20): (1190, 13)\n",
      "Shape after dropping NaNs (TP=20): (1167, 13)\n",
      "Cleaned X shape (TP=20): (1167, 12), y shape: (1167,)\n",
      "Running GridSearchCV for C and kernel (TP=20)...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best params for TP=20: {'svm__C': 10, 'svm__kernel': 'rbf'}\n",
      "Best F1 score for TP=20: 0.4522\n",
      "\n",
      "--- Overall Best Grid Search Results ---\n",
      "最佳参数组合 (Best overall parameters): {'svm__C': 1, 'svm__kernel': 'linear', 'features__timeperiod': 10}\n",
      "最佳F1分数 (Best overall F1 score): 0.5226\n",
      "\n",
      "--- Final Evaluation on Full Prepared Dataset (using best params) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.36      0.44       552\n",
      "           1       0.57      0.74      0.64       619\n",
      "\n",
      "    accuracy                           0.56      1171\n",
      "   macro avg       0.56      0.55      0.54      1171\n",
      "weighted avg       0.56      0.56      0.55      1171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tushare as ts\n",
    "import talib\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report, f1_score # 导入f1_score以便手动跟踪最佳分数\n",
    "\n",
    "# 自定义特征工程类 (接收原始价格数据，计算特征，输出包含NaN的特征)\n",
    "# 这个类现在主要在prepare_full_dataset之后、Pipeline外部使用，用于特定timeperiod的特征计算\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    接收包含原始价格数据的DataFrame，根据timeperiod计算技术指标和滞后特征。\n",
    "    计算结果会包含NaN。\n",
    "    \"\"\"\n",
    "    def __init__(self, timeperiod=14):\n",
    "        self.timeperiod = timeperiod\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df_features = df.copy()\n",
    "\n",
    "        # 技术指标计算 - 使用输入df中的原始列\n",
    "        df_features['MA20'] = talib.SMA(df_features['close'], timeperiod=20)\n",
    "        df_features['ATR'] = talib.ATR(df_features['high'], df_features['low'], df_features['close'], timeperiod=14)\n",
    "        df_features['RSI'] = talib.RSI(df_features['close'], timeperiod=self.timeperiod) # 使用可调参数timeperiod\n",
    "        df_features['CCI'] = talib.CCI(df_features['high'], df_features['low'], df_features['close'], timeperiod=20)\n",
    "\n",
    "        # 特征工程增强\n",
    "        df_features['PriceChange'] = df_features['close'].pct_change()\n",
    "        df_features['VolChange'] = df_features['vol'].pct_change()\n",
    "\n",
    "        # 滞后特征\n",
    "        for lag in [1, 2, 3]:\n",
    "            df_features[f'RSI_lag{lag}'] = df_features['RSI'].shift(lag)\n",
    "            df_features[f'VolChange_lag{lag}'] = df_features['VolChange'].shift(lag)\n",
    "\n",
    "        # 返回所有生成的特征列，丢弃原始列。\n",
    "        # 这些列会包含由talib和shift产生的NaN\n",
    "        calculated_feature_cols = [\n",
    "            'MA20', 'ATR', 'RSI', 'CCI', 'PriceChange', 'VolChange',\n",
    "            'RSI_lag1', 'RSI_lag2', 'RSI_lag3',\n",
    "            'VolChange_lag1', 'VolChange_lag2', 'VolChange_lag3'\n",
    "        ]\n",
    "\n",
    "        existing_calculated_feature_cols = [col for col in calculated_feature_cols if col in df_features.columns]\n",
    "\n",
    "        return df_features[existing_calculated_feature_cols]\n",
    "\n",
    "\n",
    "# 数据准备函数 (获取原始数据，生成标签，并清理原始NaN及标签NaN)\n",
    "# 这个函数返回的X_raw只包含清洗后对齐的原始数据，y是对应的标签\n",
    "def prepare_full_dataset(code, start, end, max_feature_lookback=20, max_label_lookahead=3, max_lag=3):\n",
    "    \"\"\"\n",
    "    下载原始股票数据，生成预测标签，并清理包含NaN的行。\n",
    "    返回的X_raw包含清洗后的原始价格数据（用于外部的FeatureEngineer），\n",
    "    返回的y是对应的标签。\n",
    "\n",
    "    Args:\n",
    "        code (str): 股票代码。\n",
    "        start (str): 开始日期 (YYYYMMDD)。\n",
    "        end (str): 结束日期 (YYYYMMDD)。\n",
    "        max_feature_lookback (int): 估计的最大技术指标回顾周期 (用于确定NaN清理的初始行数)。\n",
    "        max_label_lookahead (int): 估计的最大标签前瞻周期 (用于NaN清理)。\n",
    "        max_lag (int): 估计的最大滞后特征周期 (用于确定NaN清理的初始行数)。\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_raw, y) Pandas DataFrame 原始数据 (已清理行) 和 Pandas Series 标签集 (已清理行)，\n",
    "               如果数据获取失败或为空则返回 (None, None)。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        TUSHARE_API_KEY = os.environ.get('TUSHARE_API_KEY')\n",
    "        if not TUSHARE_API_KEY:\n",
    "             print(\"Error: TUSHARE_API_KEY environment variable not set.\")\n",
    "             return None, None\n",
    "        ts_pro_instance = ts.pro_api(TUSHARE_API_KEY)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Tushare API: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Fetching data for {code} from {start} to {end}...\")\n",
    "    try:\n",
    "        df = ts_pro_instance.daily(\n",
    "            ts_code=code,\n",
    "            start_date=start,\n",
    "            end_date=end,\n",
    "            fields='ts_code,trade_date,open,high,low,close,vol'\n",
    "        )\n",
    "        if df.empty:\n",
    "            print(f\"Error: No data retrieved for {code} from {start} to {end}\")\n",
    "            return None, None\n",
    "\n",
    "        df.sort_values(by='trade_date', ascending=True, inplace=True)\n",
    "        df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "        df.set_index('trade_date', inplace=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from Tushare: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Successfully fetched {len(df)} rows of raw data.\")\n",
    "\n",
    "    # --- 1. 标签生成 ---\n",
    "    df['future_return'] = df['close'].pct_change(max_label_lookahead).shift(-max_label_lookahead)\n",
    "    df['label'] = np.where(df['future_return'] > 0, 1, 0)\n",
    "\n",
    "    # --- 2. 确定需要删除的行范围 (基于最大的 lookback/lookahead) ---\n",
    "    # 确保数据长度足够进行特征计算和标签生成\n",
    "    # 最大的 lookback = max_feature_lookback + max_lag (e.g., 20 + 3 = 23)\n",
    "    # 最大的 lookahead = max_label_lookahead (e.g., 3)\n",
    "    # 总共需要的数据长度 = 最大的 lookback + 最大的 lookahead\n",
    "    estimated_min_length = max_feature_lookback + max_lag + max_label_lookahead\n",
    "    if len(df) < estimated_min_length:\n",
    "        print(f\"Error: Data length ({len(df)}) is too short for the specified lookbacks and lookaheads ({estimated_min_length} required).\")\n",
    "        return None, None\n",
    "\n",
    "    # 确定因为未来标签导致的 NaN 范围 (末尾几行)\n",
    "    label_na_end_index = df['label'].dropna().index[-1] if not df['label'].dropna().empty else df.index[-1]\n",
    "    # 确定因为技术指标 lookback 导致的 NaN 范围 (开头几行)\n",
    "    # 我们可以通过计算一个使用最大 lookback 的临时指标来确定第一个非 NaN 行\n",
    "    temp_check_series = talib.SMA(df['close'], timeperiod=max_feature_lookback + max_lag)\n",
    "    first_valid_index_from_features = temp_check_series.first_valid_index()\n",
    "\n",
    "    if first_valid_index_from_features is None:\n",
    "         print(\"Error: Could not calculate temporary features to determine valid start index.\")\n",
    "         return None, None\n",
    "\n",
    "    # --- 3. 数据清理 ---\n",
    "    # 删除因为技术指标 lookback 导致的开头 NaN 行\n",
    "    df_cleaned_start = df.loc[first_valid_index_from_features:].copy()\n",
    "\n",
    "    # 删除因为未来标签导致的末尾 NaN 行 (在已经清理了开头的DataFrame上操作)\n",
    "    df_cleaned_full = df_cleaned_start.dropna(subset=['label']).copy()\n",
    "\n",
    "\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    print(f\"Shape after dropping NaNs: {df_cleaned_full.shape}\")\n",
    "\n",
    "    # --- 4. 分割特征 (X_raw) 和 标签 (y) ---\n",
    "    # y 是标签列\n",
    "    y = df_cleaned_full['label']\n",
    "\n",
    "    # X_raw 包含清洗后且与y对齐的原始价格数据，丢弃标签和未来收益列\n",
    "    columns_to_drop_from_X_raw = ['label', 'future_return', 'ts_code'] # ts_code typically not used as feature\n",
    "    X_raw = df_cleaned_full.drop(columns=columns_to_drop_from_X_raw)\n",
    "\n",
    "    print(f\"Prepared X_raw shape: {X_raw.shape} (cleaned raw data)\")\n",
    "    print(f\"Prepared y shape: {y.shape}\")\n",
    "\n",
    "    return X_raw, y\n",
    "\n",
    "\n",
    "# 主程序 (手动循环 Grid Search)\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 数据准备：获取清洗后的原始数据 (X_raw) 和对齐的标签 (y_train)\n",
    "    X_train_raw, y_train = prepare_full_dataset(\n",
    "        code='600025.SH',\n",
    "        start='20200101',\n",
    "        end='20241231',\n",
    "        max_feature_lookback=20, # 用于NaN清理，确保覆盖MA/CCI的20周期和RSI的最大20周期\n",
    "        max_label_lookahead=3,   # 用于NaN清理，匹配标签计算周期\n",
    "        max_lag=3                # 用于NaN清理，匹配滞后特征数量\n",
    "    )\n",
    "\n",
    "    if X_train_raw is not None and y_train is not None:\n",
    "        print(\"\\n--- Starting Manual Grid Search across Feature Timeperiods ---\")\n",
    "\n",
    "        best_overall_score = -np.inf\n",
    "        best_overall_params = {}\n",
    "        all_timeperiods = [10, 14, 20] # Feature timeperiods to iterate over\n",
    "\n",
    "        # Smaller pipeline for scaling and SVC\n",
    "        pipeline_subset = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('svm', SVC(class_weight='balanced', random_state=42))\n",
    "        ])\n",
    "\n",
    "        # Parameter grid for the subset pipeline (only SVC params)\n",
    "        param_grid_subset = {\n",
    "            'svm__C': [0.1, 1, 10],\n",
    "            'svm__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        # Time Series Cross-Validation (使用在特定timeperiod清理后的数据上)\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "        for tp in all_timeperiods:\n",
    "            print(f\"\\nProcessing timeperiod: {tp}\")\n",
    "\n",
    "            # 2. Feature Calculation and NaN Dropping for current timeperiod\n",
    "            # 在X_train_raw上计算Features，会产生新的NaN\n",
    "            feature_engineer_tp = FeatureEngineer(timeperiod=tp)\n",
    "            X_features_tp = feature_engineer_tp.transform(X_train_raw) # 计算Features (含NaN)\n",
    "\n",
    "            # 将计算出的特征与原始对齐的y_train合并，以便同时清理X和y的对应行\n",
    "            df_combined_tp = pd.concat([X_features_tp, y_train], axis=1) # 按索引 (trade_date) 对齐\n",
    "\n",
    "            print(f\"Shape before dropping NaNs (TP={tp}): {df_combined_tp.shape}\")\n",
    "            # 删除NaN行，同时清理了X_features_tp和y_train的对应行\n",
    "            df_cleaned_tp = df_combined_tp.dropna()\n",
    "            print(f\"Shape after dropping NaNs (TP={tp}): {df_cleaned_tp.shape}\")\n",
    "\n",
    "            # 分离出为这个timeperiod清理后的 X 和 y\n",
    "            X_cleaned_tp = df_cleaned_tp.drop(columns=['label'])\n",
    "            y_cleaned_tp = df_cleaned_tp['label']\n",
    "\n",
    "            if X_cleaned_tp.empty:\n",
    "                print(f\"Warning: No samples remaining after dropping NaNs for timeperiod {tp}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Cleaned X shape (TP={tp}): {X_cleaned_tp.shape}, y shape: {y_cleaned_tp.shape}\")\n",
    "\n",
    "            # 3. 使用清理后的数据，运行 GridSearch 优化 SVC 参数\n",
    "            print(f\"Running GridSearchCV for C and kernel (TP={tp})...\")\n",
    "            grid_search_subset = GridSearchCV(\n",
    "                estimator=pipeline_subset,\n",
    "                param_grid=param_grid_subset,\n",
    "                cv=tscv, # 使用 TimeSeriesSplit 在清理后的数据上\n",
    "                scoring='f1',\n",
    "                n_jobs=-1,\n",
    "                verbose=1 # 内层循环使用较少verbose\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                # 在为当前timeperiod清理后的数据上进行拟合\n",
    "                grid_search_subset.fit(X_cleaned_tp, y_cleaned_tp)\n",
    "\n",
    "                # 4. 评估当前timeperiod的结果并更新最佳整体结果\n",
    "                print(f\"Best params for TP={tp}: {grid_search_subset.best_params_}\")\n",
    "                print(f\"Best F1 score for TP={tp}: {grid_search_subset.best_score_:.4f}\")\n",
    "\n",
    "                if grid_search_subset.best_score_ > best_overall_score:\n",
    "                    best_overall_score = grid_search_subset.best_score_\n",
    "                    # 复制当前SVC的最佳参数，并添加timeperiod参数\n",
    "                    best_overall_params = grid_search_subset.best_params_.copy()\n",
    "                    best_overall_params['features__timeperiod'] = tp # 将 timeperiod 加入最佳参数\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"An error occurred during subset GridSearchCV fit for TP={tp}: {e}\")\n",
    "                 # 可以打印 X_cleaned_tp, y_cleaned_tp 的 shapes 或 head/tail 进行调试\n",
    "\n",
    "\n",
    "        # 5. 输出整体最佳结果\n",
    "        print(\"\\n--- Overall Best Grid Search Results ---\")\n",
    "        if best_overall_score > -np.inf:\n",
    "            print(f\"最佳参数组合 (Best overall parameters): {best_overall_params}\")\n",
    "            print(f\"最佳F1分数 (Best overall F1 score): {best_overall_score:.4f}\")\n",
    "\n",
    "            # 6. 使用整体最佳参数，在对应清理后的数据上进行最终评估\n",
    "            print(\"\\n--- Final Evaluation on Full Prepared Dataset (using best params) ---\")\n",
    "\n",
    "            # 重新计算最佳timeperiod下的特征，并进行清理，获取最终用于评估的数据\n",
    "            best_tp = best_overall_params['features__timeperiod']\n",
    "            final_feature_engineer = FeatureEngineer(timeperiod=best_tp)\n",
    "            X_features_final = final_feature_engineer.transform(X_train_raw) # 在原始清洗数据上计算Features\n",
    "            df_combined_final = pd.concat([X_features_final, y_train], axis=1)\n",
    "            df_cleaned_final = df_combined_final.dropna() # 清理NaN，同时清理X和y\n",
    "            X_cleaned_final = df_cleaned_final.drop(columns=['label'])\n",
    "            y_cleaned_final = df_cleaned_final['label']\n",
    "\n",
    "            # 创建使用整体最佳SVC参数的最终Pipeline (只包含Scaler和SVC)\n",
    "            best_svm_params = {k.replace('svm__', ''): v for k, v in best_overall_params.items() if k.startswith('svm__')}\n",
    "            final_pipeline_subset = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('svm', SVC(class_weight='balanced', random_state=42, **best_svm_params))\n",
    "            ])\n",
    "\n",
    "            # 在最终清理好的数据上拟合最终模型 (不需要Cross-Validation)\n",
    "            final_pipeline_subset.fit(X_cleaned_final, y_cleaned_final)\n",
    "\n",
    "            # 使用拟合好的模型进行预测\n",
    "            predictions_final = final_pipeline_subset.predict(X_cleaned_final)\n",
    "\n",
    "            # 生成分类报告\n",
    "            print(classification_report(y_cleaned_final, predictions_final))\n",
    "\n",
    "        else:\n",
    "            print(\"Grid search failed for all timeperiods.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Data preparation failed. Exiting program.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad621e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
